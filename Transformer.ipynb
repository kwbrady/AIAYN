{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba0fe6f",
   "metadata": {},
   "source": [
    "# Contents\n",
    "[Transformer Top Level](#transformertop)\n",
    "\n",
    "> [Embedding and Positional Encoding](#emb)\n",
    "\n",
    "> [Encoder](#enc)\n",
    "\n",
    "> [Decoder](#dec)\n",
    "\n",
    "> [Multi-Head Attention](#mha)\n",
    "\n",
    "> [Inference Functions](#inffunc)\n",
    "\n",
    "[Data Management](#data)\n",
    "\n",
    "> [Multi30k Dataset](#m30k)\n",
    "\n",
    "> [Vocabularies](#vocab)\n",
    "\n",
    "> [Datapipe Collation and Masking](#mask)\n",
    "\n",
    "[Training Loop](#train)\n",
    "\n",
    "> [Top Level](#epochs)\n",
    "\n",
    "> [Optimization](#opt)\n",
    "\n",
    "> [Manual Loss](#loss)\n",
    "\n",
    "[Run Model](#run)\n",
    "\n",
    "> [Initialization](#init)\n",
    "\n",
    "> [Training](#modeltrain)\n",
    "\n",
    "> [Inference](#modelinf)\n",
    "\n",
    "[Evaluation](#eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e91432",
   "metadata": {},
   "source": [
    "# TRANSFORMER TOP LEVEL<a class=\"anchor\" id=\"transformertop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e8658",
   "metadata": {},
   "source": [
    "Note: Docstrings will use the following dimensional nomenclature throughout this notebook\n",
    "\n",
    "B  = batch size <br>\n",
    "L  = sequence length (Ls for source length and Lt for target where applicable) <br>\n",
    "E  = embedding dimension <br>\n",
    "H  = number of attention heads <br>\n",
    "V = source vocabulary size (Vs for source vocabulary and Vt for target where applicable)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d498819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The top-level class defining a transformer\n",
    "    \n",
    "    Attributes:\n",
    "        dmodel (int): The dimensionality of the transformer's embeddings\n",
    "        vocab_en (torchtext Vocab): The vocabulary object created from our English training inputs\n",
    "        vocab_de (torchtext Vocab): The vocabulary object created from our German training targets\n",
    "        pedrop (torch Module): A dropout layer following positional encoding\n",
    "        en_embedding (torch Module): The input embedding layer for English sentences to be translated\n",
    "        de_embedding (torch Module): The input embedding layer for German translations\n",
    "        encoder (torch Module): The top-level class implementing the encoder stack\n",
    "        decoder (torch Module): The top-level class implementing the decoder stack\n",
    "        generator (torch Module): The linear layer following the decoder; outputs to softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_en, vocab_de, dmodel=512, dff=2048, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer initialization function \n",
    "        Default values of input arguments follow \"Attention is All You Need\" (AIAYN)\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            dff (int): The dimmensionality of feed-forward layers within the encoder and decoder\n",
    "            num_heads (int): The number of attention heads used in both self- and cross-attention layers\n",
    "            num_layers (int): The number of times encoder and decoder layers are repeated within their top-level stacks\n",
    "            dropout (float): The value used for dropout layers throughout the transformer\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert dmodel % 2 == 0, 'The embedding dimension must be an even number.'\n",
    "        \n",
    "        self.dmodel = dmodel\n",
    "        \n",
    "        self.pedrop = nn.Dropout(dropout) # For use after the positional encoding\n",
    "        self.embedding_en = ScaledEmbedding(len(vocab_en),dmodel)\n",
    "        self.embedding_de = ScaledEmbedding(len(vocab_de),dmodel)\n",
    "        self.encoder = Encoder(dmodel, dff, num_heads, num_layers, dropout)\n",
    "        self.decoder = Decoder(dmodel, dff, num_heads, num_layers, dropout)\n",
    "        self.generator = nn.Linear(dmodel, len(vocab_de))\n",
    "        self.generator.weight = self.embedding_de.embedding.weight # Matches the final linear layer's weights to those of the embedding layer \n",
    "        \n",
    "    def forward(self, batch_en, mask_en, batch_de, mask_de):\n",
    "        \"\"\"\n",
    "        Transformer forward step\n",
    "        \n",
    "        Args:\n",
    "            batch_en (torch Tensor): A 2D tensor of dim = (B,Ls) made up of English sentences, tokenized and indexed to a vocabulary\n",
    "            mask_en (torch Tensor): A 4D training mask of dim = (B,1,1, Ls) used to ignore sentence padding in attention calculations\n",
    "            batch_de (torch Tensor): A 2D tensor of dim = (B,Lt) made up of German translations, tokenized and indexed to a vocabulary\n",
    "            mask_de (torch Tensor): A 4D training mask of dim = (B,1,Lt,Lt) used in attention calculations, made up of a padding mask and an upper triangular look-ahead mask\n",
    "            output_probs (boolean): Toggles the return value between the outputs of the Transformer's final linear layer (when False), and the post-softmax probabilities (when True)\n",
    "\n",
    "        Returns:\n",
    "            A tensor of dim (B,Lt,Vt) which contains either pre- or post-softmax transformer output values, depending on the value of output_probs\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len_en = batch_en.size(1)\n",
    "        seq_len_de = batch_de.size(1)\n",
    "\n",
    "        x_enc = self.embedding_en(batch_en) # Convert incoming English batch to embedding\n",
    "\n",
    "        x_enc = x_enc + positional_encoding_matrix(seq_len_en, self.dmodel)\n",
    "        x_enc = self.pedrop(x_enc)\n",
    "        x_enc = self.encoder(x_enc, mask_en) # Pass embedding with positional encoding and mask to encoder\n",
    "        \n",
    "        x = self.embedding_de(batch_de) # Convert incoming German batch to embedding\n",
    "        x = x + positional_encoding_matrix(seq_len_de, self.dmodel)\n",
    "        x = self.pedrop(x)\n",
    "        x = self.decoder(x, x_enc, mask_en, mask_de) # Pass embedding with positional encoding and mask to decoder along with encoder output\n",
    "        \n",
    "        x = self.generator(x)\n",
    "        \n",
    "        return x \n",
    "    \n",
    "    def translate(self, sentence, tokenizer_en, vocab_en, tokenizer_de, vocab_de, max_sentence_len=100):\n",
    "        \"\"\"\n",
    "        Carries out a forward step, then passes it through a log_softmax and returns the translated sentence\n",
    "        \n",
    "        Currently, this function only supports string-to-string translation, though I may add list handling for batches of strings in the future\n",
    "        \n",
    "        Args:\n",
    "            sentence (str): The English sentence to be translated\n",
    "            tokenizer_en: An instance of spacy.lang.en.English\n",
    "            vocab_en (torchtext Vocab): The English language vocabulary\n",
    "            tokenizer_de: An instance of spacy.lang.de.German\n",
    "            vocab_de (torchtext Vocab): The German language vocabulary\n",
    "            max_sentence_len (int): The maximum allowed length of a translated sentence\n",
    "            \n",
    "        Returns:\n",
    "            The English sentence's German translation\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval()\n",
    "        EoS_output = vocab_de(['<EoS>'])\n",
    "        \n",
    "        source = pre_process(sentence, tokenizer_en, vocab_en)\n",
    "        \n",
    "        output = ''\n",
    "        output = pre_process(output, tokenizer_de, vocab_de, source=False)\n",
    "        \n",
    "        current_token = output\n",
    "        next_token = torch.zeros(1,1,dtype=torch.int) \n",
    "        \n",
    "        for _ in range(max_sentence_len):\n",
    "        \n",
    "            fwdpass = self.forward(source,None,output,None)\n",
    "\n",
    "            log_probs = F.log_softmax(fwdpass, dim=2)\n",
    "\n",
    "            next_token = gen_next_token(log_probs) # Find the token with the highes log probability\n",
    "    \n",
    "            output = torch.cat((output,next_token),1)\n",
    "        \n",
    "            # Break loop if <EoS> is generated\n",
    "            if (next_token[0,0]) == EoS_output[0]:\n",
    "                break\n",
    "        \n",
    "        tokens_str = [vocab_de.get_itos()[x] for x in output[0,:]] # Look up tokens in vocabulary\n",
    "        output_str = ' '.join(tokens_str) # List to string\n",
    "        \n",
    "        # Remove <SoS> and <EoS> tokens \n",
    "        if output_str == '<SoS> <EoS>':\n",
    "            return ''\n",
    "        else:\n",
    "            output_str = output_str.replace('<SoS> ','').replace(' <EoS>','') \n",
    "            \n",
    "        self.train()\n",
    "\n",
    "        return output_str\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Re-initializes all parameters throughout the transformer\n",
    "        \"\"\"\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf82c55",
   "metadata": {},
   "source": [
    "## Embedding and Positional Encoding <a class=\"anchor\" id=\"emb\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard embedding class with a multiplier, as defined in AIAYN\n",
    "    \n",
    "    Attributes:\n",
    "        dmodel (int): The dimensionality of the embedding\n",
    "        embedding (torch Module): A standard PyTorch Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_len, dmodel):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            vocab_len (int): The length of the vocabulary to which the embedding is mapped\n",
    "            dmodel (int): The dimensionality of the embedding \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dmodel = dmodel\n",
    "        self.embedding = nn.Embedding(vocab_len, dmodel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward step\n",
    "            \n",
    "        Args:\n",
    "            x (torch Tensor): Batch of sentences, tokenized and indexed to the vocabulary, dim = (B,L)\n",
    "        \n",
    "        Returns:\n",
    "            A tensor of dim (B,L,E) containing the scaled embedding of the input batch\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.embedding(x)*math.sqrt(self.dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437460f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_matrix(seq_len, dmodel, denom_base=10000.):\n",
    "    \"\"\"\n",
    "    Creates a positional encoding matrix to be added to an input sequence\n",
    "        \n",
    "    Args:\n",
    "        seq_len (int): The length of the input sequence\n",
    "        dmodel (int): The dimensionality of the word embedding\n",
    "        denom_base (float): A number that appears as the base of an exponential in the denominator of the encoding angle\n",
    "\n",
    "    Returns:\n",
    "        The positional encoding as a tensor of dim=(S,E)\n",
    "    \"\"\"\n",
    "    \n",
    "    encmat = torch.zeros(seq_len, dmodel)\n",
    "    \n",
    "    posvec = torch.arange(seq_len) # Position-tracking vector\n",
    "    posvec = posvec.unsqueeze(1)\n",
    "\n",
    "    idxmat = torch.ones(seq_len, dmodel//2) / torch.pow(denom_base,torch.arange(0,dmodel,2)/dmodel) # Index iterator\n",
    "\n",
    "    encmat[:,0:dmodel:2] = torch.sin(posvec * idxmat)\n",
    "    encmat[:,1:dmodel:2] = torch.cos(posvec * idxmat)\n",
    "\n",
    "    return encmat.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138720f5",
   "metadata": {},
   "source": [
    "## ENCODER<a class=\"anchor\" id=\"enc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11975590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The top-level class defining an Encoder\n",
    "    \n",
    "    Attributes:\n",
    "        encoder_layers (torch ModuleList): A PyTorch ModuleList containing 'num_layers' EncoderLayer modules\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, dff, num_heads, num_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            dff (int): The dimmensionality of encoder feed-forward layers\n",
    "            num_heads (int): The number of heads used in self-attention layers\n",
    "            num_layers (int): The number of times encoder layers are repeated within the top-level stack \n",
    "            dropout (float): The probability provided to each encoder layer's dropout sublayer\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([copy.deepcopy(EncoderLayer(dmodel, dff, num_heads, dropout)) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Encoder forward step\n",
    "            \n",
    "        Args:\n",
    "            x (torch Tensor): The transformer's English source input tensor, dim=(B,Ls,E)\n",
    "            mask (torch Tensor): The training mask to be applied; expected to be a padding mask of dim = (B,1,1,Ls) where 1=padding\n",
    "        \n",
    "        Returns:\n",
    "            The encoder output, a tensor of dim=(B,Ls,E) to be used in decoder cross-attention calculations\n",
    "        \"\"\"\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the encoder, to be used in the Encoder class\n",
    "    \n",
    "    Attributes:\n",
    "        MHA (torch Module): The multi-head self-attention layer\n",
    "        ff1 (torch Module): The first of the position-wise feed-forward layers (expands)\n",
    "        ff2 (torch Module): The second of the position-wise feed-forward layers (reduces)\n",
    "        drop (torch Module): The dropout layer following ff2\n",
    "        lnormMHA (torch Module): A PyTorch LayerNorm module, to be applied after the dropout layer and the adding of residuals\n",
    "        lnormff (torch Module): A PyTorch LayerNorm module, to be applied after the dropout layer and the adding of residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, dff, num_heads, dropout):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            dff (int): The dimmensionality of encoder feed-forward layers\n",
    "            num_heads (int): The number of heads used in self-attention layers\n",
    "            dropout (float): The dropout probability provided to self.drop\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.MHA = ManualMHA(dmodel, num_heads)\n",
    "        self.ff1 = nn.Linear(dmodel,dff)\n",
    "        self.ff2 = nn.Linear(dff,dmodel)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lnormMHA = nn.LayerNorm(dmodel)\n",
    "        self.lnormff = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        \"\"\"\n",
    "        Forward step\n",
    "        \n",
    "        Note that this function follows a different norm/add sequence than AIAYN. The correct ordering seems to be a matter of open discussion and this is what worked well empirically.\n",
    "            \n",
    "        Args:\n",
    "            x (torch Tensor): Input - English source tensor or the output of a previous encoder layer, dim=(B,Ls,E)\n",
    "            mask (torch Tensor): The training mask to be applied; expected to be a padding mask of dim = (B,1,1,Ls) where 1=padding\n",
    "        \n",
    "        Returns:\n",
    "            Encoder sublayer output - a tensor of dim=(B,Ls,E) to be sent to the following encoder layer or to the decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        xres = x \n",
    "        x = self.lnormMHA(x)\n",
    "        x = xres + self.drop(self.MHA(x,x,x,mask=mask)) # Multi-head attention, with dropout and residual addition\n",
    "        \n",
    "        xres = x\n",
    "        x = self.lnormff(x)\n",
    "        \n",
    "        x = self.ff1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = xres + self.drop(self.ff2(x)) # Feed forward, with dropout and residual addition\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789de8ed",
   "metadata": {},
   "source": [
    "## DECODER<a class=\"anchor\" id=\"dec\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99571410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The top-level class defining an Decoder\n",
    "    \n",
    "    Attributes:\n",
    "        decoder_layers (torch ModuleList): A PyTorch ModuleList containing 'num_layers' DecoderLayer modules\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, dff, num_heads, num_layers, dropout):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            dff (int): The dimmensionality of decoder feed-forward layers\n",
    "            num_heads (int): The number of heads used in self- and cross-attention layers\n",
    "            num_layers (int): The number of times decoder layers are repeated within the top-level stack \n",
    "            dropout (float): The probability provided to each decoder layer's dropout sublayer\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([copy.deepcopy(DecoderLayer(dmodel, dff, num_heads, dropout)) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, output_enc, mask_enc, mask_dec):\n",
    "        \"\"\"\n",
    "        Decoder forward step\n",
    "            \n",
    "        Args:\n",
    "            x (torch Tensor): The transformer's German target input tensor, dim=(B,Lt,E)\n",
    "            output_enc (torch Tensor): The output of the encoder, to be used in cross-attention, dim=(B,Ls,E)\n",
    "            mask_enc (torch Tensor): The mask to be applied to the encoder output, expected to be a padding mask of dim = (B,1,1,Ls) where 1=padding\n",
    "            mask_dec (torch Tensor): The mask to be applied to the target, expected to be a padding mask of dim = (B,1,Lt,Lt) where 1=padding\n",
    "        \n",
    "        Returns:\n",
    "            The decoder output, a tensor of dim=(B,Lt,E) to be passed to the final linear and softmax layers\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, output_enc, mask_enc, mask_dec)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the decoder, to be used in the Decoder class\n",
    "    \n",
    "    Attributes:\n",
    "        MHA_self (torch Module): The multi-head self-attention layer\n",
    "        MHA_cross (torch Module): The multi-head encoder cross-attention layer\n",
    "        ff1 (torch Module): The first of the position-wise feed-forward layers (expands)\n",
    "        ff2 (torch Module): The second of the position-wise feed-forward layers (reduces)\n",
    "        drop (torch Module): The dropout layer following ff2\n",
    "        lnormMHA (torch Module): A PyTorch LayerNorm module, to be applied after the dropout layer and the adding of residuals\n",
    "        lnormff (torch Module): A PyTorch LayerNorm module, to be applied after the dropout layer and the adding of residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, dff, num_heads, dropout):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            dff (int): The dimmensionality of decoder feed-forward layers\n",
    "            num_heads (int): The number of heads used in self- and cross-attention layers\n",
    "            num_layers (int): The number of times decoder layers are repeated within the top-level stack \n",
    "            dropout (float): The probability provided to each decoder layer's dropout sublayer\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.MHA_self = ManualMHA(dmodel, num_heads)\n",
    "        self.MHA_cross = ManualMHA(dmodel, num_heads)\n",
    "        self.ff1 = nn.Linear(dmodel,dff)\n",
    "        self.ff2 = nn.Linear(dff,dmodel)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.lnormMHA = nn.LayerNorm(dmodel)\n",
    "        self.lnormff = nn.LayerNorm(dmodel)\n",
    "        \n",
    "    def forward(self, x, output_enc, mask_enc, mask_dec):\n",
    "        \"\"\"\n",
    "        Forward step\n",
    "            \n",
    "        Args:\n",
    "            x: Input - German target tensor or the output of a previous decoder layer, dim=(B,Lt,E)\n",
    "            output_enc: The output of the encoder, to be used in all cross-attention calculations, dim=(B,Ls,E)\n",
    "            mask_enc: The mask to be applied to the encoder output, expected to be a padding mask of dim = (B,1,1,Ls) where 1=padding\n",
    "            mask_dec: The mask to be applied to the target, expected to be a padding mask of dim = (B,1,Lt,Lt) where 1=padding\n",
    "        \n",
    "        Returns:\n",
    "            Decoder sublayer output - a tensor of dim=(B,Lt,E) sent to the following decoder layer or to the final linear and softmax layers\n",
    "        \"\"\"\n",
    "\n",
    "        xres = x \n",
    "        x = self.lnormMHA(x)\n",
    "        x = xres + self.drop(self.MHA_self(x,x,x,mask=mask_dec)) # Multi-head attention, with dropout and residual addition\n",
    "        \n",
    "        xres = x \n",
    "        x = self.lnormMHA(x)\n",
    "        x = xres + self.drop(self.MHA_cross(x,output_enc,output_enc,mask=mask_enc)) # Multi-head attention, with dropout and residual addition\n",
    "        \n",
    "        xres = x\n",
    "        x = self.lnormff(x)\n",
    "        \n",
    "        x = self.ff1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = xres + self.drop(self.ff2(x)) # Feed forward, with dropout and residual addition\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75104d3",
   "metadata": {},
   "source": [
    "## MULTI-HEAD ATTENTION<a class=\"anchor\" id=\"mha\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualMHA(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-head Attention (MHA), used in both self- and cross-attention calculations within the encoder and decoder\n",
    "    \n",
    "    Attributes:\n",
    "        dmodel (int): The dimensionality of the transformer's embeddings\n",
    "        num_heads (int): The number of attention heads \n",
    "        wQ (torch Module): A PyTorch Linear module with weights mapping from input to queries, to be split across heads\n",
    "        wK (torch Module): A PyTorch Linear module with weights mapping from input to keys, to be split across heads\n",
    "        wV (torch Module): A PyTorch Linear module with weights mapping from input to values, to be split across heads\n",
    "        wO (torch Module): The matrix mapping from the concatenated outputs of all attention heads to the final MHA output value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, num_heads):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        \n",
    "        Args:\n",
    "            dmodel (int): The dimensionality of the transformer's embeddings\n",
    "            num_heads (int): The number of attention heads \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert dmodel % num_heads == 0\n",
    "        \n",
    "        self.dmodel = dmodel\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.wQ = nn.Linear(dmodel, dmodel, bias=False) # The concatenation of QKV mappings across all heads, to be split up in the forward pass\n",
    "        self.wK = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.wV = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        \n",
    "        self.wO = nn.Linear(dmodel, dmodel, bias=False) # The concatenation of attention mappings from all heads, to be provided to the output\n",
    "        \n",
    "    def forward(self, xQ, xK, xV, mask=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward step, performs the attention calculation\n",
    "        If xQ == xK == xV, this is self-attention\n",
    "    \n",
    "             \n",
    "        Args:\n",
    "            xQ: The input to be multiplied by wQ to create the query tensor, xQ dim=(B,L,E)\n",
    "            xK: The input to be multiplied by wK to create the key tensor, xK dim=(B,L,E)\n",
    "            xV: The input to be multiplied by wV to create the value tensor, xV dim=(B,L,E)\n",
    "            mask: The mask to be applied to the argument of the softmax function, dimensionality differs according to self- and cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            A tensor of attention values to be sent to the next sublayer of the encoder or decoder, dim=(B,L,E)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = xQ.size(0)\n",
    "        max_seq_Q = xQ.size(1) # The length of the maximum sequence in the provided data\n",
    "        max_seq_KV = xK.size(1) # This will be equivalent to max_seq_Q for self-attention calculations, but not necessarily for cross attention\n",
    "        \n",
    "        dQKV = self.dmodel // self.num_heads\n",
    "\n",
    "        Q = self.wQ(xQ)\n",
    "        Q = Q.contiguous().view(batch_size, max_seq_Q, self.num_heads, dQKV)\n",
    "        Q = Q.permute(0,2,1,3)\n",
    "        \n",
    "        K = self.wK(xK)\n",
    "        K = K.contiguous().view(batch_size, max_seq_KV, self.num_heads, dQKV)\n",
    "        K = K.permute(0,2,1,3)\n",
    "        \n",
    "        V = self.wV(xV)\n",
    "        V = V.contiguous().view(batch_size, max_seq_KV, self.num_heads, dQKV)\n",
    "        V = V.permute(0,2,1,3)\n",
    "        \n",
    "        sm_arg = torch.matmul(Q, K.transpose(2,3))/math.sqrt(dQKV) # Argument to softmax function\n",
    "        \n",
    "        if mask!=None:\n",
    "            mask.to(xQ.device) # This line is not necessary as long as I'm running everything on CPU, but a useful placeholder for possible future work\n",
    "            sm_arg = sm_arg - mask*1e9\n",
    "\n",
    "        sm = F.softmax(sm_arg, dim = -1) # Takes softmax across rows of the Q, K^T matrix product\n",
    "        \n",
    "        attn = torch.matmul(sm, V) # Attention matrix, split across heads\n",
    "\n",
    "        attn_cat = torch.flatten(attn.permute(0,2,1,3), start_dim = -2) # Concatenate the values for all heads\n",
    "        \n",
    "        attn_out = self.wO(attn_cat)\n",
    "\n",
    "        return attn_out            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8af71b",
   "metadata": {},
   "source": [
    "## INFERENCE FUNCTIONS<a class=\"anchor\" id=\"inffunc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(sentence, tokenizer, vocab, source=True):\n",
    "    \"\"\"\n",
    "    Converts an incoming sentence to a set of vocabulary-indexed tokens for use in inference.\n",
    "        \n",
    "    Args:\n",
    "        sentence (str): The source sentence to be translated, ignored if source=False\n",
    "        tokenizer: An instance of spacy.lang.en.English or spacy.lang.de.German\n",
    "        vocab (torchtext Vocab): The vocabulary for use in indexing\n",
    "        source (boolean): If True, assumes an English sentence and returns the tokenized, indexed version. If False, returns the German <SoS> token.\n",
    "\n",
    "    Returns:\n",
    "        A list of indexed token values\n",
    "    \"\"\"\n",
    "    \n",
    "    pstrip = str.maketrans('', '', string.punctuation) # Used to strip punctuation from incoming sentences\n",
    "    sentence = sentence.rstrip('\\n').translate(pstrip).lower()\n",
    "    \n",
    "    if source:\n",
    "        tokens = ['<SoS>'] + [tok.text for tok in tokenizer.tokenizer(sentence)] + ['<EoS>']\n",
    "    else:\n",
    "        tokens = ['<SoS>']\n",
    "\n",
    "        \n",
    "    tokens_indexed = vocab(tokens)\n",
    "    \n",
    "    tensor_out = torch.tensor(tokens_indexed, dtype=torch.int)\n",
    "    tensor_out = tensor_out.unsqueeze(0)\n",
    "    \n",
    "    return tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_next_token(probs):\n",
    "    \"\"\"\n",
    "    Selects a value to append to a transformer output during inference using greedy decoding\n",
    "        \n",
    "    Args:\n",
    "        probs (torch Tensor): The post-softmax output of a transformer, dim=(1,L,Vt)\n",
    "        \n",
    "    Returns:\n",
    "        The highest-probability next token\n",
    "    \"\"\"\n",
    "\n",
    "    index = torch.argmax(probs[0,-1,:])\n",
    "    \n",
    "    # Add two dimensions for agreement with transformer output format\n",
    "    index = index.unsqueeze(0)\n",
    "    index = index.unsqueeze(0)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8996063",
   "metadata": {},
   "source": [
    "# DATA MANAGEMENT<a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f02644",
   "metadata": {},
   "source": [
    "I'll be using Multi30k for English-to-German translation. This isn't a very large dataset and, since the aim of this project is familiarizing myself with transformer structure, I'll just load it all into memory rather than trying to set up lazy loading. I may revisit in the future.\n",
    "\n",
    "There is a pre-built PyTorch Multi30k datapipe, but for the sake of understanding I'll create and organize the dataset myself in the ManualMulti30k class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581681f",
   "metadata": {},
   "source": [
    "## MULTI30K DATASET<a class=\"anchor\" id=\"m30k\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d28444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualMulti30k(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a Dataset from a subset (train, validation, or test) of Multi30k data.\n",
    "    \n",
    "    Attributes:\n",
    "        data_en (list): A list of English sentences\n",
    "        data_de (list): A list of German translations that match the sentences of data_en\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, data_spec):\n",
    "        \"\"\"\n",
    "        Given a source directory and a specified subset of data, initializes a ManualMulti30k object\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): A directory containing appropriate plaintext files with Multi30k data; specific formatting of those files is expected; see comments below\n",
    "            data_spec (str): Accepts 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        \n",
    "        data_spec = data_spec.lower()\n",
    "        assert data_spec == 'train' or data_spec == 'val' or data_spec == 'test'\n",
    "\n",
    "        path_en = os.path.join(data_dir, data_spec + '.en') # This follows the file naming convention of the data used by quest.dcs.shef.ac.uk/wmt16_files_mmt/\n",
    "        path_de = os.path.join(data_dir, data_spec + '.de')\n",
    "        \n",
    "        pstrip = str.maketrans('', '', string.punctuation) # Used to strip punctuation from incoming sentences\n",
    "        \n",
    "        with open(path_en) as file:\n",
    "            self.data_en = [line.rstrip('\\n').translate(pstrip).lower() for line in file.readlines()]\n",
    "            \n",
    "        with open(path_de) as file:\n",
    "            self.data_de = [line.rstrip('\\n').translate(pstrip).lower() for line in file.readlines()]\n",
    "\n",
    "        assert len(self.data_en) == len(self.data_de)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a pair of English and German sentences\n",
    "        \n",
    "        Args:\n",
    "            idx (int): The index of the desired sentence pair\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (English sentence, German translation)\n",
    "        \"\"\"\n",
    "        \n",
    "        sentence_en = self.data_en[idx]\n",
    "        sentence_de = self.data_de[idx]\n",
    "        \n",
    "        return sentence_en, sentence_de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a118890",
   "metadata": {},
   "source": [
    "## VOCABULARIES<a class=\"anchor\" id=\"vocab\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabularies(dataset, tokenizer_en, idx_en, tokenizer_de, idx_de):\n",
    "    \"\"\"\n",
    "    Returns the English and German vocabularies\n",
    "        \n",
    "    Args:\n",
    "        dataset (torch ConcatDataset): A dataset containing all tuples of (English sentence, German translation) to be included in the vocabulary\n",
    "        tokenizer_en: An instance of spacy.lang.en.English\n",
    "        idx_en (int): The index of the English sentence within the tuple\n",
    "        tokenizer_de: An instance of spacy.lang.de.German\n",
    "        idx_de (int): The index of the German translation within the tuple\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (English, German) torchtext Vocabularies\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_en = build_vocab_from_iterator(\n",
    "        token_gen(iter(dataset), tokenizer_en, idx_en),\n",
    "        min_freq=2,\n",
    "        specials=[\"<pad>\", \"<SoS>\", \"<EoS>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_de = build_vocab_from_iterator(\n",
    "        token_gen(iter(dataset), tokenizer_de, idx_de),\n",
    "        min_freq=2,\n",
    "        specials=[\"<pad>\", \"<SoS>\", \"<EoS>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "    vocab_de.set_default_index(vocab_de[\"<unk>\"])\n",
    "\n",
    "    return vocab_en, vocab_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_gen(data_iter, tokenizer, language_idx):\n",
    "    \"\"\"\n",
    "    A generator that yields tokenized versions of a sentence\n",
    "        \n",
    "    Args:\n",
    "        data_iter (iterator): An iterator over a ManualMulti30k dataset\n",
    "        tokenizer: A Spacy tokenizer (spacy.lang.en.English or spacy.lang.de.German)\n",
    "        language_idx (int): The element of the tuple (English sentence, German translation) to be tokenized\n",
    "\n",
    "    Yields:\n",
    "        A list of tokens from the selected sentence\n",
    "    \"\"\"\n",
    "\n",
    "    for sentence_pair in data_iter:\n",
    "        sentence = sentence_pair[language_idx]\n",
    "        yield [tok.text for tok in tokenizer.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fc27b",
   "metadata": {},
   "source": [
    "## DATAPIPE COLLATION AND MASKING<a class=\"anchor\" id=\"mask\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7650e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, tokenizer_en, vocab_en, vocab_de, tokenizer_de):\n",
    "    \"\"\"\n",
    "    A DataLoader collate_fn that tokenizes and pads the incoming batch\n",
    "        \n",
    "    Args:\n",
    "        batch (list): A list of (English, German) sentence pair tuples as provided by the ManualMulti30k class\n",
    "        tokenizer_en: An instance of spacy.lang.en.English\n",
    "        vocab_en (torchtext Vocab): The English language vocabulary\n",
    "        tokenizer_de: An instance of spacy.lang.de.German\n",
    "        vocab_de (torchtext Vocab): The German language vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        A batch of data in the form of two tensors (English, German) of tokenized values \n",
    "    \"\"\"\n",
    "    \n",
    "    batch_len = len(batch)\n",
    "    seq_len_en = 1 # This variable tracks the maximum length of a sentence in the batch\n",
    "    seq_len_de = 1\n",
    "    \n",
    "    en_out = torch.zeros(batch_len, seq_len_en, dtype=torch.int) \n",
    "    de_out = torch.zeros(batch_len, seq_len_de, dtype=torch.int) \n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for (en_sentence, de_sentence) in batch:\n",
    "        \n",
    "        en_tokens = [\"<SoS>\"] + [str(token) for token in tokenizer_en(en_sentence)] + [\"<EoS>\"]\n",
    "        en_processed = torch.Tensor(vocab_en(en_tokens)) # Numericalize the tokenized sentence\n",
    "\n",
    "        if len(en_processed) > seq_len_en: # If the last sentence processed is longer than any seen so far, pad the output tensors accordingly\n",
    "            padding = len(en_processed) - seq_len_en\n",
    "            seq_len_en = len(en_processed)\n",
    "            en_out = F.pad(en_out, (0,padding), \"constant\", 0)\n",
    "            \n",
    "        en_out[idx,:len(en_processed)] = en_processed # Assign the numericalized data to a row of the output tensor\n",
    " \n",
    "        de_tokens = [\"<SoS>\"] + [str(token) for token in tokenizer_de(de_sentence)] + [\"<EoS>\"]\n",
    "        de_processed = torch.Tensor(vocab_de(de_tokens)) # Numericalize the tokenized sentence\n",
    "        \n",
    "        if len(de_processed) > seq_len_de: # If the last sentence processed is longer than any seen so far, pad the output tensors accordingly\n",
    "            padding = len(de_processed) - seq_len_de\n",
    "            seq_len_de = len(de_processed)\n",
    "            de_out = F.pad(de_out, (0,padding), \"constant\", 0) \n",
    "            \n",
    "        de_out[idx,:len(de_processed)] = de_processed # Assign the numericalized data to a row of the output tensor\n",
    "        \n",
    "        idx += 1\n",
    "    \n",
    "    mask_en = create_padding_mask(en_out)\n",
    "    \n",
    "    mask_de = create_padding_mask(de_out) + create_lookahead_mask(seq_len_de)\n",
    "    mask_de = (mask_de!=0).int() # Correction for the previous line, which introduced values >1 in areas of overlap\n",
    "    \n",
    "    return (en_out, mask_en, de_out, mask_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b13253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_tens):\n",
    "    \"\"\"\n",
    "    Creates and applies a padding mask to be used with input to a multi-head attention's softmax function\n",
    "    \n",
    "    Args:\n",
    "        input_tens (torch Tensor): The tensor for which the mask will be used, dim=(B,L)\n",
    "    \n",
    "    Returns:\n",
    "        mask: A tensor of dim=(B,1,1,Ls) that masks padded values (padding=1)\n",
    "    \"\"\"\n",
    "\n",
    "    mask = (input_tens==0).int()\n",
    "    mask = mask.unsqueeze(1) # Add a dimension to be used in an outer product with a second sequence\n",
    "    mask = mask.unsqueeze(1) # Add a dimension to broadcast across heads\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa185256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookahead_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Creates a look-ahead mask that keeps a decoder from using later words in a sequence in its attention calculations\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): The length of the longest sequence in the batch to be masked\n",
    "    \n",
    "    Returns:\n",
    "        mask: A tensor of dim=(B,1,Lt,Lt) that masks the final two dimensions with an upper triangular matrix (masking=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = torch.triu(torch.ones(seq_len,seq_len), diagonal=1) # Upper triangular matrix to replace the inner two (equal) dimensions of the input\n",
    "    \n",
    "    mask = mask.unsqueeze(0) # Add two dimensionts broadcast across batches and heads\n",
    "    mask = mask.unsqueeze(0)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89813ee5",
   "metadata": {},
   "source": [
    "# TRAINING LOOP<a class=\"anchor\" id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ae62d",
   "metadata": {},
   "source": [
    "## TOP LEVEL<a class=\"anchor\" id=\"epochs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(transformer, train_dl, valid_dl, criterion, optimizer, scheduler, num_epochs, batch_reporting=10, clip_value=100., monitoring=False):\n",
    "    \"\"\"\n",
    "    Executes the entire training process for our transformer model\n",
    "    \n",
    "    Args:\n",
    "        transformer (torch Module): The transformer to be trained\n",
    "        train_dl (torch DataLoader): The training data's DataLoader\n",
    "        valid_dl (torch DataLoader): The validation data's DataLoader\n",
    "        criterion (torch Module): A module storing the loss function and associated information\n",
    "        optimizer (torch Adam): The Adam optimizer\n",
    "        scheduler (torch lr_scheduler.LambdaLR): The learning rate scheduler\n",
    "        num_epochs (int): The number of epochs to execute \n",
    "        batch_reporting (int): The number of batches to execute within an epoch before printing a status update\n",
    "        clip_value (float): The max norm value to which all gradients will be clipped \n",
    "        monitoring (boolean): If True, a sample source, target, and model output will be printed as part of the batch_reporting status update\n",
    "    \"\"\"\n",
    "    \n",
    "    dt_now = datetime.datetime.now()\n",
    "    dt_now = dt_now.strftime(\"%Y%m%d_%H%M\")\n",
    "    savedir = f\"Training_Run_{dt_now}\"\n",
    "    \n",
    "    # Create directory with date/timestamp for saving state dict after each epoch\n",
    "    \n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    \n",
    "    # Begin training run\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        tstart = time.time()\n",
    "        \n",
    "        print(f\"Epoch {i}, beginning training run\")\n",
    "        transformer.train()\n",
    "        train_loss = single_epoch(transformer, train_dl, criterion, optimizer, scheduler, batch_reporting, clip_value=clip_value, train_mode=True, monitoring=monitoring)\n",
    "        \n",
    "        print(f\"Epoch {i}: beginning validation run\")\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = single_epoch(transformer, valid_dl, criterion, optimizer, scheduler, batch_reporting, monitoring=monitoring)\n",
    "        \n",
    "        trun = time.time() - tstart\n",
    "        \n",
    "        print(f\"Epoch {i}: Training loss={train_loss}, Validation loss={valid_loss}, Epoch Runtime={trun}\")\n",
    "        print()\n",
    "        \n",
    "        saveloc = os.path.join(savedir, f\"Epoch_{i}_Statedict.pt\")\n",
    "        \n",
    "        torch.save(transformer.state_dict(),saveloc)\n",
    "        \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00929b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_epoch(transformer, data_dl, criterion, optimizer, scheduler, batch_reporting, train_mode=False, clip_value=100, monitoring=False):\n",
    "    \"\"\"\n",
    "    Handles execution of a single epoch.\n",
    "    \n",
    "    Args:\n",
    "        transformer (torch Module): The transformer to be trained\n",
    "        data_dl (torch DataLoader): The training or validation data's DataLoader\n",
    "        criterion (torch Module): A module storing the loss function and associated information\n",
    "        optimizer (torch Adam): The Adam optimizer\n",
    "        scheduler (torch lr_scheduler.LambdaLR): The learning rate scheduler\n",
    "        batch_reporting (int): The number of batches to execute within an epoch before printing a status update \n",
    "        train_mode (boolean): If True, the transformer will update its weights during the epoch\n",
    "        clip_value (float): The max norm value to which all gradients will be clipped \n",
    "        monitoring (boolean): If True, a sample source, target, and model output will be printed as part of the batch_reporting status update\n",
    "    \n",
    "    Returns:\n",
    "        The loss calculation for the epoch as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    batchnum = 0\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    for batch in data_dl:\n",
    "        \n",
    "        batch_en = batch[0]\n",
    "        mask_en = batch[1] \n",
    "        batch_de = batch[2]\n",
    "        mask_de = batch[3]\n",
    "\n",
    "        outputs = transformer(batch_en, mask_en, batch_de, mask_de)\n",
    "        outputs = outputs[:,:-1,:]\n",
    "        \n",
    "        vocab_size = outputs.size(-1)\n",
    "        outputs_to_loss = outputs.contiguous().view(-1, vocab_size) # Resize into a 2D matrix that can be pased to CrossEntropyLoss\n",
    "        \n",
    "        target = batch_de[:,1:]\n",
    "        target_to_loss = target.contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(outputs_to_loss, target_to_loss.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if train_mode:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value) # I found that this worked well in later-stage training\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batchnum % batch_reporting == 0:\n",
    "            trun = time.time() - tstart\n",
    "            print(f\"Batch {batchnum} complete, runtime {trun} sec\")\n",
    "\n",
    "            if monitoring:\n",
    "                source_sentence = batch_en[0]\n",
    "                target_sentence = batch_de[0]\n",
    "                \n",
    "                source_sentence_tokens = [vocab_en.get_itos()[x] for x in source_sentence] # Look up tokens in English vocabulary\n",
    "                target_sentence_tokens = [vocab_de.get_itos()[x] for x in target_sentence] # Look up tokens in German vocabulary\n",
    "                \n",
    "                log_probs = F.log_softmax(outputs, dim=2) # Conversion from model output to probabilities\n",
    "                next_tokens = torch.argmax(log_probs,2) # Most likely next token values\n",
    "                \n",
    "                output_tokens = [vocab_de.get_itos()[x] for x in next_tokens[0,:]] \n",
    "                target_tokens = [vocab_de.get_itos()[x] for x in target[0,:]]\n",
    "                \n",
    "                print()\n",
    "                print(f\" - Monitoring Enabled - \")\n",
    "                if train_mode:\n",
    "                    print(f\"Max Gradient Norm: {max([param.grad.data.norm(2).item()for param in model.parameters()])}\")\n",
    "                print(f\"Source from data: {source_sentence_tokens}\")\n",
    "                print(f\"Taget from data: {target_sentence_tokens}\")\n",
    "                print(f\"Model output: {output_tokens}\")\n",
    "                print(f\"Target tokens: {target_tokens}\")\n",
    "                print()\n",
    "            \n",
    "            tstart = time.time()\n",
    "        \n",
    "        batchnum += 1\n",
    "        \n",
    "    # This is averaged across sentences rather than tokens, so the train loss can wind up being continually higher than the validation loss.\n",
    "    average_loss = total_loss/len(data_dl)\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4dc3e",
   "metadata": {},
   "source": [
    "## OPTIMIZATION<a class=\"anchor\" id=\"opt\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb52447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_scheduler(transformer, dmodel, start_step=0, warmup_steps=4000, betas=(0.9,0.98), eps=1e-9):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler for the Adam optimizer\n",
    "        \n",
    "    Args:\n",
    "        transformer (torch Module): The transformer linked to the optimizer\n",
    "        dmodel (int): The dimensionality of the transformer's embeddings\n",
    "        start_step (int): The step at which to start the scheduler\n",
    "        warmup_steps (int): The step at which the schedule shifts from a linearly increasing learning rate to a decreasing one\n",
    "        betas (tuple) = Coefficients of the Adam algorithm\n",
    "        eps (float) = Small value used in an Adam calculation's denominator for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        An instance of torch.optim.lr_scheduler.LambdaLR\n",
    "    \"\"\"\n",
    "    \n",
    "    lfunc = lambda x: learning_rate(dmodel,x + start_step, warmup_steps) \n",
    "    \n",
    "    scheduler = opt.lr_scheduler.LambdaLR(optimizer, lr_lambda=lfunc, verbose=False)\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(dmodel, step_num, warmup_steps):\n",
    "    \"\"\"\n",
    "    The AIAYN learning rate calculation for the Adam optimizer\n",
    "        \n",
    "    Args:\n",
    "        dmodel (int): The dimensionality of the transformer's embeddings\n",
    "        step_num (int): The current step's index\n",
    "        warmup_steps (int): The step at which the schedule shifts from a linearly increasing learning rate to a decreasing one\n",
    "\n",
    "    Returns:\n",
    "        The rate value calculated based on the current step\n",
    "        \n",
    "    \"\"\"\n",
    "    if step_num == 0:\n",
    "        return dmodel**-0.5 * step_num * warmup_steps**-1.5\n",
    "    else:\n",
    "        rate = dmodel**(-0.5) * min(step_num**(-0.5), step_num * warmup_steps**(-1.5))\n",
    "\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_optimizer(transformer, betas=(0.9,0.98), eps=1e-9):\n",
    "    \"\"\"\n",
    "    Create the Adam optimizer\n",
    "    \n",
    "    Args:\n",
    "        transformer (torch Module): The transformer to be linked to the optimizer\n",
    "        betas (tuple) = Coefficients of the Adam algorithm \n",
    "        eps (float) = Small value used in an Adam calculation's denominator for numerical stability\n",
    "        \n",
    "    Returns:\n",
    "        An instance of torch.optim.Adam\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = opt.Adam(transformer.parameters(), lr=1, betas=betas, eps=eps) # Learning rate will be overwritten by scheduler\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c774d",
   "metadata": {},
   "source": [
    "## MANUAL LOSS<a class=\"anchor\" id=\"loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ManualCEL(logits, target, smoothing=0.1, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    An explicitly defined cross-entropy loss, unused in this script but previously compared to the PyTorch implementation for debugging\n",
    "        \n",
    "    Args:\n",
    "        logits (torch Tensor): A tensor of dim=(B*Lt,Vt) containing a pre-softmax transformer output\n",
    "        target (torch Tensor): A tensor of dim=(B*Lt) containing vocabulary indices for each token in a batch of sentences\n",
    "        smoothing (float): The amount of label smoothing to apply to the calculation\n",
    "\n",
    "    Returns:\n",
    "        The cross-entropy loss calculation as a float\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = logits.size(-1)\n",
    "    \n",
    "    target = F.one_hot(target, num_classes=vocab_size)\n",
    "\n",
    "    target_reduce = target * (1 - smoothing - smoothing/(vocab_size-1)) # Apply label smoothing: first line\n",
    "    target_smooth = target_reduce + torch.ones(target.size())*smoothing/(vocab_size-1) # Apply label smoothing: second line\n",
    "    \n",
    "    probs = F.softmax(logits,dim=-1) # Conversion of logits to probabilities\n",
    "    probs = probs.detach()\n",
    "\n",
    "    probs = probs.apply_(lambda x: max(1e-9, min(1 - 1e-9, x)))\n",
    "    \n",
    "    crossentropy = - target_smooth * torch.log(probs)\n",
    "\n",
    "    return torch.sum(crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7bfbc",
   "metadata": {},
   "source": [
    "# RUN MODEL<a class=\"anchor\" id=\"run\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaed14",
   "metadata": {},
   "source": [
    "## INITIALIZATION<a class=\"anchor\" id=\"init\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c416aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_idx = 0 # Track the ordering of the sentence-pair tuples\n",
    "de_idx = 1\n",
    "\n",
    "# Create train, val, and test Datasets.\n",
    "\n",
    "userdir = os.path.expanduser('~')\n",
    "datadir = os.path.join(userdir, '.data/')\n",
    "\n",
    "train_dataset = ManualMulti30k(datadir,'train')\n",
    "val_dataset = ManualMulti30k(datadir,'val')\n",
    "test_dataset = ManualMulti30k(datadir,'test')\n",
    "\n",
    "combined_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset]) # For building vocabularies\n",
    "\n",
    "# Import tokenizers from Spacy\n",
    "\n",
    "try:\n",
    "    tokenizer_en = spacy.load('en_core_web_sm')\n",
    "except IOError:\n",
    "    os.system('python -m spacy download en_core_web_sm')\n",
    "    tokenizer_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "try:\n",
    "    tokenizer_de = spacy.load('de_core_news_sm')\n",
    "except IOError:\n",
    "    os.system('python -m spacy download de_core_news_sm')\n",
    "    tokenizer_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Create English and German vocabularies\n",
    "    \n",
    "print(\"Preparing Data\")\n",
    "vocab_en, vocab_de = build_vocabularies(combined_dataset, tokenizer_en, en_idx, tokenizer_de, de_idx)\n",
    "\n",
    "vocab_len_en = len(vocab_en)\n",
    "vocab_len_de = len(vocab_de)\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True, collate_fn=partial(collate, tokenizer_en=tokenizer_en, vocab_en = vocab_en, tokenizer_de=tokenizer_de, vocab_de = vocab_de))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=partial(collate, tokenizer_en=tokenizer_en, vocab_en = vocab_en, tokenizer_de=tokenizer_de, vocab_de = vocab_de))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Data Preparation Complete\")\n",
    "print(f\"English vocabulary length: {vocab_len_en}\")\n",
    "print(f\"German vocabulary length: {vocab_len_de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Transformer\n",
    "\n",
    "weight_import = f\"Trained_Weights.pt\" # Change value to None to initialize a new model\n",
    "\n",
    "model = Transformer(vocab_en, vocab_de)\n",
    "\n",
    "if weight_import != None:\n",
    "    model.load_state_dict(torch.load(weight_import))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7694047",
   "metadata": {},
   "source": [
    "## TRAINING<a class=\"anchor\" id=\"modeltrain\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apart from the gradient clip value, the numbers below correspond to the AIAYN defaults. In practice I had to do some experimenting to get good training results\n",
    "\n",
    "num_epochs = 100\n",
    "warmup_steps = 4000\n",
    "start_step = 0\n",
    "clip_value = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = gen_optimizer(model)\n",
    "scheduler = gen_scheduler(model,model.dmodel, start_step=start_step, warmup_steps=warmup_steps)\n",
    "\n",
    "train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs, batch_reporting=50, clip_value=clip_value, monitoring=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290791ee",
   "metadata": {},
   "source": [
    "## INFERENCE<a class=\"anchor\" id=\"modelinf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09372f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_input = 'The cow jumped over the moon.'\n",
    "\n",
    "test_output = model.translate(test_input, tokenizer_en, vocab_en, tokenizer_de, vocab_de)\n",
    "\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c074f3",
   "metadata": {},
   "source": [
    "# EVALUATION<a class=\"anchor\" id=\"eval\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bae89e",
   "metadata": {},
   "source": [
    "Note that using the default Multi30k test set to calculate BLEU score can take a long time. You might consider using just a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c43e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computes the BLEU score for transformer output over the Multi30k test Dataset - this takes a few minutes to run\n",
    "\n",
    "translations = []\n",
    "references = []\n",
    "\n",
    "pstrip = str.maketrans('', '', string.punctuation) # Used to strip punctuation from incoming sentences\n",
    "idx=0\n",
    "print(f'Calculating BLEU score')\n",
    "for sentence_pair in test_dataloader:\n",
    "    try:\n",
    "        idx += 1\n",
    "        if idx % 10 == 0:\n",
    "            print(f'{idx}/{len(test_dataloader)} sentences evaluated')\n",
    "        en_sentence = sentence_pair[0][0].translate(pstrip).lower()\n",
    "        de_translation = model.translate(en_sentence, tokenizer_en, vocab_en, tokenizer_de, vocab_de)\n",
    "        de_translation_tokens = [str(token) for token in tokenizer_de(de_translation)]\n",
    "\n",
    "        de_reference = sentence_pair[1][0].translate(pstrip).lower()\n",
    "        de_reference_tokens = [str(token) for token in tokenizer_de(de_reference)]\n",
    "\n",
    "        translations.append(de_translation_tokens)\n",
    "        references.append([de_reference_tokens])\n",
    "    except:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "    \n",
    "print(f'Finished. BLEU score = {bleu_score(translations,references)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
